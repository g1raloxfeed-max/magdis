import cv2
import torch
from transformers import CLIPProcessor, CLIPModel
from elasticsearch import Elasticsearch, helpers
import time
import sys
import os

# =================== –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ===================
VIDEO_FILE_PATH = "–ø—É—Ç—å_–∫_–≤–∞—à–µ–º—É_–≤–∏–¥–µ–æ—Ñ–∞–π–ª—É.mp4"  # <-- –ó–ê–ú–ï–ù–ò–¢–ï –ù–ê –°–í–û–ô –ü–£–¢–¨
ELASTICSEARCH_HOST = "http://localhost:9200"
INDEX_NAME = "video_frames"
FRAME_EXTRACTION_INTERVAL = 1  # –ë—Ä–∞—Ç—å –∫–∞–¥—Ä –∫–∞–∂–¥—É—é —Å–µ–∫—É–Ω–¥—É
CLIP_MODEL_NAME = "openai/clip-vit-base-patch32"  # –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å, —Ö–æ—Ä–æ—à–∏–π –±–∞–ª–∞–Ω—Å —Å–∫–æ—Ä–æ—Å—Ç–∏/–∫–∞—á–µ—Å—Ç–≤–∞
# ===================================================

# 1. –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –ö–õ–ò–ï–ù–¢–û–í
print("üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤...")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Elasticsearch
es = Elasticsearch(ELASTICSEARCH_HOST)
if not es.ping():
    print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Elasticsearch!")
    sys.exit(1)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è CLIP –º–æ–¥–µ–ª–∏ (–∑–∞–≥—Ä—É–∑–∫–∞ –Ω–∞ GPU, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

model = CLIPModel.from_pretrained(CLIP_MODEL_NAME).to(device)
processor = CLIPProcessor.from_pretrained(CLIP_MODEL_NAME)
model.eval()  # –ü–µ—Ä–µ–≤–æ–¥–∏–º –º–æ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å: {CLIP_MODEL_NAME}")

# 2. –°–û–ó–î–ê–ù–ò–ï –ò–ù–î–ï–ö–°–ê (–ï–°–õ–ò –ù–ï –°–£–©–ï–°–¢–í–£–ï–¢)
index_mapping = {
    "mappings": {
        "properties": {
            "video_name": {"type": "keyword"},
            "frame_number": {"type": "integer"},
            "timestamp_sec": {"type": "float"},
            "frame_vector": {
                "type": "dense_vector",
                "dims": 512,  # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–∞ –¥–ª—è clip-vit-base-patch32
                "index": True,
                "similarity": "cosine"
            },
            "file_path": {"type": "keyword"}
        }
    },
    "settings": {
        "number_of_shards": 1,
        "number_of_replicas": 0
    }
}

if not es.indices.exists(index=INDEX_NAME):
    es.indices.create(index=INDEX_NAME, body=index_mapping)
    print(f"‚úÖ –°–æ–∑–¥–∞–Ω –∏–Ω–¥–µ–∫—Å: {INDEX_NAME}")
else:
    print(f"‚ÑπÔ∏è  –ò–Ω–¥–µ–∫—Å '{INDEX_NAME}' —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")

# 3. –û–ë–†–ê–ë–û–¢–ö–ê –í–ò–î–ï–û –ò –ò–ù–î–ï–ö–°–ê–¶–ò–Ø
print(f"\nüé• –ù–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É –≤–∏–¥–µ–æ: {VIDEO_FILE_PATH}")

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Ñ–∞–π–ª–∞
if not os.path.exists(VIDEO_FILE_PATH):
    print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {VIDEO_FILE_PATH}")
    sys.exit(1)

# –û—Ç–∫—Ä—ã–≤–∞–µ–º –≤–∏–¥–µ–æ
cap = cv2.VideoCapture(VIDEO_FILE_PATH)
if not cap.isOpened():
    print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –≤–∏–¥–µ–æ—Ñ–∞–π–ª!")
    sys.exit(1)

# –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–∏–¥–µ–æ
fps = cap.get(cv2.CAP_PROP_FPS)
total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
duration = total_frames / fps
video_name = os.path.basename(VIDEO_FILE_PATH)

print(f"üìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≤–∏–¥–µ–æ:")
print(f"   ‚Ä¢ FPS: {fps:.2f}")
print(f"   ‚Ä¢ –í—Å–µ–≥–æ –∫–∞–¥—Ä–æ–≤: {total_frames}")
print(f"   ‚Ä¢ –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {duration:.2f} —Å–µ–∫")
print(f"   ‚Ä¢ –ö–∞–¥—Ä–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: ~{int(duration / FRAME_EXTRACTION_INTERVAL)}")

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –±–∞—Ç—á–µ–π –¥–∞–Ω–Ω—ã—Ö
def generate_documents():
    frame_count = 0
    processed_count = 0
    start_time = time.time()
    
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–∞–¥—Ä—ã —Å –∑–∞–¥–∞–Ω–Ω—ã–º –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–º
        current_time_sec = frame_count / fps
        if frame_count % int(fps * FRAME_EXTRACTION_INTERVAL) == 0:
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∫–∞–¥—Ä –∏–∑ BGR (OpenCV) –≤ RGB (–¥–ª—è CLIP)
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –º–æ–¥–µ–ª–∏ CLIP
            inputs = processor(images=frame_rgb, return_tensors="pt", padding=True)
            inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∫–∞–¥—Ä–∞
            with torch.no_grad():
                image_features = model.get_image_features(**inputs)
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ numpy –º–∞—Å—Å–∏–≤ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–µ–∫—Ç–æ—Ä
            vector = image_features.cpu().numpy().flatten().astype('float32')
            
            # –°–æ–∑–¥–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –¥–ª—è Elasticsearch
            doc = {
                "_index": INDEX_NAME,
                "_source": {
                    "video_name": video_name,
                    "frame_number": frame_count,
                    "timestamp_sec": round(current_time_sec, 2),
                    "frame_vector": vector.tolist(),  # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å–ø–∏—Å–æ–∫ –¥–ª—è JSON
                    "file_path": VIDEO_FILE_PATH
                }
            }
            
            processed_count += 1
            if processed_count % 10 == 0:
                elapsed = time.time() - start_time
                print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∫–∞–¥—Ä–æ–≤: {processed_count} ({(processed_count/elapsed):.1f} –∫–∞–¥—Ä/—Å–µ–∫)")
            
            yield doc
        
        frame_count += 1
    
    cap.release()
    print(f"\n‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    print(f"   ‚Ä¢ –í—Å–µ–≥–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–µ–Ω–æ –∫–∞–¥—Ä–æ–≤: {frame_count}")
    print(f"   ‚Ä¢ –í–µ–∫—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω–æ –∫–∞–¥—Ä–æ–≤: {processed_count}")
    print(f"   ‚Ä¢ –û–±—â–µ–µ –≤—Ä–µ–º—è: {time.time() - start_time:.1f} —Å–µ–∫")

# 4. –ò–ù–î–ï–ö–°–ê–¶–ò–Ø –í ELASTICSEARCH –° –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï–ú BULK API
print("\nüì§ –û—Ç–ø—Ä–∞–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ Elasticsearch...")
try:
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º bulk-–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
    success, failed = helpers.bulk(
        es,
        generate_documents(),
        chunk_size=50,  # –†–∞–∑–º–µ—Ä –ø–∞—á–∫–∏ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏
        request_timeout=30,
        max_retries=3
    )
    
    print(f"‚úÖ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    print(f"   ‚Ä¢ –£—Å–ø–µ—à–Ω–æ: {success} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    print(f"   ‚Ä¢ –û—à–∏–±–æ–∫: {len(failed) if failed else 0}")
    
    # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏–Ω–¥–µ–∫—Å–∞
    if success > 0:
        es.indices.refresh(index=INDEX_NAME)
        count = es.count(index=INDEX_NAME)['count']
        print(f"   ‚Ä¢ –í—Å–µ–≥–æ –≤ –∏–Ω–¥–µ–∫—Å–µ: {count} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {e}")
    cap.release()

# 5. –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê
print("\n" + "="*50)
print("–ò–ù–î–ï–ö–°–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê")
print("="*50)
print(f"–ò–Ω–¥–µ–∫—Å: {INDEX_NAME}")
print(f"–í–∏–¥–µ–æ—Ñ–∞–π–ª: {video_name}")
print(f"–°–µ—Ä–≤–µ—Ä Elasticsearch: {ELASTICSEARCH_HOST}")